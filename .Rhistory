setwd("/Users/alessandrocapelli/Desktop/Progetto ML")
setwd("/Users/alessandrocapelli/Desktop/Progetto\ ML")
setwd("/Users/alessandrocapelli/Desktop/Progetto ML")
install.packages(c("caret", "mlbench", "rpart", "rpart.plot", "randomForest", "rattle", "RColorBrewer", "corrplot", "class", "FactoMineR", "factoextra", "e1071", "neuralnet"))
library(caret)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(RColorBrewer)
library(corrplot)
library(class)
library(FactoMineR)
library(factoextra)
library(e1071)
library(neuralnet)
library(rattle)
dataset <- read.csv("Dataset.csv", stringsAsFactors = T, sep = ',', header = TRUE)
dataset <- dataset[-c(1:2,23)]
colnames(dataset)
dataset$RainToday = ifelse(dataset$RainToday=="No", 0, 1)
dataset$RainTomorrow = ifelse(dataset$RainTomorrow=="No", 0, 1)
dataset$RainToday = factor(dataset$RainToday)
dataset$RainTomorrow = factor(dataset$RainTomorrow)
#INIZIO A VEDERE LA STRUTTURA GENERALE DEL DATASET
str(dataset)
head(dataset)
for (i in c(1:ncol(dataset))){
print(paste(colnames(dataset[i]), "| Number of NA: ", sum(is.na(dataset[i])), "| % of NA: ", sum(is.na(dataset[i]))/nrow(dataset)*100))
}
dataset = dataset[-c(4:5, 16:17)]
colnames(dataset)
dataset = na.omit(dataset)
str(dataset)
head(dataset, n = 20)
for (i in c(1:ncol(dataset))){
print(paste(colnames(dataset[i]), "| Number of NA: ", sum(is.na(dataset[i])), "| % of NA: ", sum(is.na(dataset[i]))/nrow(dataset)*100))
}
M <- cor(dataset[-c(4, 6:7, 16:17)])
M
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # AGGIUNGE COEFFICENTE DI CORRELAZIONE
tl.col="black", tl.srt=45, # COLORE E ROTAZIONE DELLA LABEL
# NASCONDE LA CORRELAZIONE NELLA DIAGONALE PRINCIPALE (TUTTA = A 1)
diag=F
)
highlyCorrelated <- findCorrelation(M, cutoff = 0.67, names = TRUE)
highlyCorrelated
dataset <- dataset[-c(2, 5, 10, 13:15)]
View(dataset)
colnames(dataset)
View(M)
HELP(findCorrelation())
help(findCorrelation())
help(findCorrelation
)
setwd("/Users/alessandrocapelli/Desktop/Progetto ML")
install.packages(c("caret", "mlbench", "rpart", "rpart.plot", "randomForest", "rattle", "RColorBrewer", "corrplot", "class", "FactoMineR", "factoextra", "e1071", "neuralnet"))
library(caret)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(RColorBrewer)
library(corrplot)
library(class)
library(FactoMineR)
library(factoextra)
library(e1071)
library(neuralnet)
library(caret)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(RColorBrewer)
library(corrplot)
library(class)
library(FactoMineR)
library(factoextra)
library(e1071)
library(neuralnet)
#CARICO IL DATASET COMPLETO
dataset <- read.csv("Dataset.csv", stringsAsFactors = T, sep = ',', header = TRUE)
dataset <- dataset[-c(1:2,23)]
colnames(dataset)
dataset$RainToday = ifelse(dataset$RainToday=="No", 0, 1)
dataset$RainTomorrow = ifelse(dataset$RainTomorrow=="No", 0, 1)
#dataset$WindGustDir = as.numeric(factor(dataset$WindGustDir))
dataset$RainToday = factor(dataset$RainToday)
dataset$RainTomorrow = factor(dataset$RainTomorrow)
#INIZIO A VEDERE LA STRUTTURA GENERALE DEL DATASET
str(dataset)
head(dataset)
for (i in c(1:ncol(dataset))){
print(paste(colnames(dataset[i]), "| Number of NA: ", sum(is.na(dataset[i])), "| % of NA: ", sum(is.na(dataset[i]))/nrow(dataset)*100))
}
dataset = dataset[-c(4:5, 16:17)]
colnames(dataset)
#ORA AVENDO OSSERVATO CHE LE ALTRE NON HANNO UN NUMERO DI NA COSI' SIGNIFICATIVO LA STRATEGIA PIU' VELOCE E
# CHE PERMETTE DI NON INTRODURRE ASSUNZIONI ESTERNE E' OMETTERE TUTTE LE OSSERVAZIONI CHE CONTENGONO NA
dataset = na.omit(dataset)
#IL DATASET PASSA PERO' DA 142193 OSSERVAZIONI A 112925 (ERANO PRESENTI QUINDI CIRCA 30000 OSSERVAZIONI CHE CONTENEVANO NA)
#ORA LA FUNZIONE STR MI INDICA ANCHE QUALI OSSERVAZIONI SONO STATE RIMOSSE ATTRAVERSO LA NA.OMIT (PER ESEMPIO
#L'OSSERVAZIONE NUMERO 15)
str(dataset)
head(dataset, n = 20)
for (i in c(1:ncol(dataset))){
print(paste(colnames(dataset[i]), "| Number of NA: ", sum(is.na(dataset[i])), "| % of NA: ", sum(is.na(dataset[i]))/nrow(dataset)*100))
}
M <- cor(dataset[-c(4, 6:7, 16:17)])
M
#brewer.pal(n=8, name="RdBu")
#method = color, number o circle
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col="black", # AGGIUNGE COEFFICENTE DI CORRELAZIONE
tl.col="black", tl.srt=45, # COLORE E ROTAZIONE DELLA LABEL
# NASCONDE LA CORRELAZIONE NELLA DIAGONALE PRINCIPALE (TUTTA = A 1)
diag=F
)
highlyCorrelated <- findCorrelation(M, cutoff = 0.67, names = TRUE)
highlyCorrelated
#FACCIO UNA COPIA DEL DATASET, PRIMA DI RIMUOVERE GLI ATTRIBUTI, PER POTER EFFETTUARE PCA
datasetPCA <- dataset
#RIMUOVO GLI ATTRIBUTI ("Temp9am", "MaxTemp", "Temp3pm", "Pressure3pm", "Humidity9am", "WindGustSpeed")
dataset <- dataset[-c(2, 5, 10, 13:15)]
#VISUALIZZO LE COLONNE DEL DATASET RIDOTTO
colnames(dataset)
View(datasetPCA)
help("findCorrelation")
highlyCorrelated <- findCorrelation(M, cutoff = 0.67, names = TRUE, verbose = TRUE)
res.pca <- PCA(datasetPCA[-c(4, 6:7, 16:17)], scale.unit = TRUE, graph = FALSE)
View(res.pca)
eig.val <- get_eigenvalue(res.pca)
eig.val
View(dataset)
View(datasetPCA)
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
var <- get_pca_var(res.pca)
var$cor
var$contrib
fviz_pca_var(res.pca, col.var = "black")
ind <- get_pca_ind(res.pca)
ind
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col="black", # AGGIUNGE COEFFICENTE DI CORRELAZIONE
tl.col="black", tl.srt=45, # COLORE E ROTAZIONE DELLA LABEL
diag=F # NASCONDE LA CORRELAZIONE NELLA DIAGONALE PRINCIPALE (TUTTA = A 1)
)
set.seed(123)
ind = sample(2, nrow(dataset), replace = T,  prob = c(0.7, 0.3))
trainset = dataset[ind == 1,]
testset = dataset[ind == 2,]
table(trainset$RainTomorrow)
prop.table(table(trainset$RainTomorrow))
table(testset$RainTomorrow)
prop.table(table(testset$RainTomorrow))
help("sample")
ind
plot(trainset$RainTomorrow, names = c("No", "Yes"), col=c("red", "green"))
table(trainset$RainTomorrow, trainset$RainToday)
prop.table(table(trainset$RainToday, trainset$RainTomorrow),1)
barplot(table(trainset$RainTomorrow, trainset$RainToday), col=c("red", "green"), names = c("No", "Yes"), legend=c("No","Yes"))
testset$Prediction = 0
testset$Prediction = factor(testset$Prediction)
confMatrix <- table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy: ", sum(diag(confMatrix))/sum(confMatrix)))
confusionMatrix(testset$Prediction, testset$RainTomorrow)
testset$Prediction = 0
testset$Prediction[testset$RainToday == 1] = 1
testset$Prediction = factor(testset$Prediction)
confusionMatrix(testset$Prediction, testset$RainTomorrow)
start_time <- Sys.time()
bigDecisionTree = rpart(RainTomorrow ~ .,
data = trainset, method = "class", control = rpart.control(cp = 0))
end_time <- Sys.time()
print(end_time - start_time)
plot(bigDecisionTree)
plotcp(bigDecisionTree)
start_time <- Sys.time()
testset$Prediction <- predict(bigDecisionTree, testset, type = "class")
end_time <- Sys.time()
print(end_time - start_time)
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Big Decision Tree: ", sum(diag(confMatrix))/sum(confMatrix)))
start_time <- Sys.time()
decisionTree = rpart(RainTomorrow ~ .,
data = trainset, method = "class", control = rpart.control(cp = 0, maxdepth = 6))
end_time <- Sys.time()
print(end_time - start_time)
plot(decisionTree)
plotcp(decisionTree)
start_time <- Sys.time()
testset$Prediction <- predict(decisionTree, testset, type = "class")
end_time <- Sys.time()
print(end_time - start_time)
#CALCOLO LE PERFORMANCE DEL MODELLO
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Decision Tree: ", sum(diag(confMatrix))/sum(confMatrix)))
start_time <- Sys.time()
prunedDecisionTree = prune(decisionTree, cp = 0.017)
end_time <- Sys.time()
print(end_time - start_time)
#ESSENDO MOLTO PIU' RIDOTTO POSSO VISUALIZZARLO IN MODO MIGLIORE
fancyRpartPlot(prunedDecisionTree)
#VEDO CHE IL MODELLO CREATO UTILIZZA AL MASSIMO 3 ATTRIBUTI PER ASSOCIARE UN ETICHETTA ALLE OSSERVAZIONI,
#VOGLIO PERO' CONTROLLARE L'ACCURACY CHE MI GARANTISCE QUESTO MODELLO "SEMPLIFICATO"
help("fancyRpartPlot")
testset$Prediction <- predict(prunedDecisionTree, testset, type = "class", main = "")
fancyRpartPlot(prunedDecisionTree, main = "")
help("fancyRpartPlot")
fancyRpartPlot(prunedDecisionTree, sub = "")
start_time <- Sys.time()
testset$Prediction <- predict(prunedDecisionTree, testset, type = "class")
end_time <- Sys.time()
print(end_time - start_time)
#CALCOLO LE PERFORMANCE DEL MODELLO
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Pruned Decision Tree: ", sum(diag(confMatrix))/sum(confMatrix)))
#HO UNA ACCURACY DEL 83% CIRCA E' LEGGERMENTE MINORE RISPETTO AL MODELLO PRECEDENTE (84%) MA MOLTO MIGLIORE IN TERMINI
#DI COMPLESSITA' E QUINDI E' IL MODELLO DA PREFERIRE!!
#ALLENO ORA UNA RANDOM FOREST
start_time <- Sys.time()
randomForest = randomForest(RainTomorrow ~ .,
data = trainset, method = "class", ntree = 500)
end_time <- Sys.time()
print(end_time - start_time)
#CREO LA PREVISIONE UTILIZZANDO IL MODELLO ALLENATO
start_time <- Sys.time()
testset$Prediction <- predict(randomForest, testset, type = "class")
end_time <- Sys.time()
print(end_time - start_time)
#CALCOLO LE PERFORMANCE DEL MODELLO
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Random Forest: ", sum(diag(confMatrix))/sum(confMatrix)))
#SI PUO' NOTARE PERO' CHE IL MODELLO E' PIU' PRECISO ARRIVANDO A SFIORARE L'85% DI ACCURACY
#MODELLO 2: NAIVE BAYES
start_time <- Sys.time()
naiveBayes = naiveBayes(trainset, trainset$RainTomorrow)
end_time <- Sys.time()
print(end_time - start_time)
#CREO LA PREVISIONE UTILIZZANDO IL MODELLO ALLENATO
start_time <- Sys.time()
testset$Prediction <- predict(naiveBayes, testset, type = "class")
end_time <- Sys.time()
print(end_time - start_time)
#CALCOLO LE PERFORMANCE DEL MODELLO
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Naive Bayes: ", sum(diag(confMatrix))/sum(confMatrix)))
#NAIVE BAYES MI DA UN ACCURACY ALTISSIMA, SUPERIORE AL 99%
wordcloud(trainset$WindGustDir, min.freq=0, random.order = FALSE)
library(wordcloud)
install.packages(wordcloud)
install.packages("wordcloud")
library(wordcloud)
wordcloud(trainset$WindGustDir, min.freq=0, random.order = FALSE)
install.packages("tm", "slam")
library(wordcloud)
wordcloud(trainset$WindGustDir)
start_time <- Sys.time()
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
end_time <- Sys.time()
print(end_time - start_time)
plot(nn)
View(trainset)
start_time <- Sys.time()
testset$Prediction <- compute(nn, testset)
end_time <- Sys.time()
print(end_time - start_time)
#CALCOLO LE PERFORMANCE DEL MODELLO
confMatrix = table(testset$Prediction, testset$RainTomorrow)
confMatrix
print(paste("Accuracy Neural Network: ", sum(diag(confMatrix))/sum(confMatrix)))
library(doParallel)
install.package("doParallel")
install.packages("doParallel")
library(doParallel)
remove(nn)
start_time <- Sys.time()
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
plot(nn)
remove(nn)
start_time <- Sys.time()
cl <- makePSOCKcluster(2)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
remove(nn)
start_time <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
remove(nn)
start_time <- Sys.time()
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
remove(nn)
start_time <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
start_time <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
start_time <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
start_time <- Sys.time()
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
View(cl)
remove(nn)
remove(cl)
remove(start_time)
remove(end_time)
start_time <- Sys.time()
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
nn = neuralnet(RainTomorrow ~ MinTemp + Rainfall + WindSpeed9am + WindSpeed3pm + Humidity3pm + Pressure9am,
trainset, hidden = length(dataset))
stopCluster(cl)
end_time <- Sys.time()
print(end_time - start_time)
install.packages(c("caret", "mlbench", "rpart", "rpart.plot", "randomForest", "rattle", "RColorBrewer", "corrplot", "class", "FactoMineR", "factoextra", "e1071", "neuralnet", "doParallel"))
library(caret)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(RColorBrewer)
library(corrplot)
library(class)
library(FactoMineR)
library(factoextra)
library(e1071)
library(neuralnet)
library(doParallel)
dataset <- read.csv("Dataset.csv", stringsAsFactors = T, sep = ',', header = TRUE)
View(dataset)
View(dataset)
for (i in c(1:ncol(dataset))){
print(paste(colnames(dataset[i]), "| Number of NA: ", sum(is.na(dataset[i])), "| % of NA: ", sum(is.na(dataset[i]))/nrow(dataset)*100))
}
